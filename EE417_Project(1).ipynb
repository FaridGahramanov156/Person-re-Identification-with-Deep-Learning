{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Step 1: Imports and Dataset Setup\n",
    "import os\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Paths (adjust if needed)\n",
    "DATA_DIR = \"data/Market-1501\"\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"bounding_box_train\")\n",
    "TEST_DIR = os.path.join(DATA_DIR, \"bounding_box_test\")\n",
    "QUERY_DIR = os.path.join(DATA_DIR, \"query\")\n",
    "\n",
    "# Basic transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 128)),  # standard ReID input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Market1501Dataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_paths = sorted(glob(os.path.join(img_dir, '*.jpg')))\n",
    "        self.transform = transform\n",
    "\n",
    "        # Parse raw person IDs\n",
    "        self.raw_labels = []\n",
    "        for path in self.img_paths:\n",
    "            filename = os.path.basename(path)\n",
    "            person_id = int(filename.split('_')[0])\n",
    "            cam_id = int(filename.split('c')[1][0])\n",
    "            self.raw_labels.append((person_id, cam_id))\n",
    "\n",
    "        # Normalize person IDs to 0-based indexing\n",
    "        unique_ids = sorted(set([pid for pid, _ in self.raw_labels]))\n",
    "        self.id_map = {pid: idx for idx, pid in enumerate(unique_ids)}\n",
    "\n",
    "        # Final labels\n",
    "        self.labels = [(self.id_map[pid], cam_id) for pid, cam_id in self.raw_labels]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        person_id, cam_id = self.labels[idx]\n",
    "        return img, person_id, cam_id\n"
   ],
   "id": "131b1bf16444dfc9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 3: Load Dataset and Preview\n",
    "train_dataset = Market1501Dataset(TRAIN_DIR, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Preview\n",
    "images, labels, cams = next(iter(train_loader))\n",
    "print(\"Batch of images:\", images.shape)\n",
    "print(\"Person IDs:\", labels[:10])\n",
    "print(\"Camera IDs:\", cams[:10])\n",
    "\n",
    "# Optional: Visualize a few samples\n",
    "plt.figure(figsize=(12, 4))\n",
    "for i in range(6):\n",
    "    plt.subplot(1, 6, i+1)\n",
    "    img = images[i].permute(1, 2, 0).numpy()\n",
    "    img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])  # unnormalize\n",
    "    img = np.clip(img, 0, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"ID: {labels[i].item()}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "f5cb40772fabd196",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Verify normalized IDs\n",
    "all_ids = [label for _, label, _ in train_dataset]\n",
    "unique_ids = sorted(set(all_ids))\n",
    "\n",
    "print(\"Total unique person IDs:\", len(unique_ids))\n",
    "print(\"First 10 IDs:\", unique_ids[:10])\n",
    "print(\"Last 10 IDs:\", unique_ids[-10:])\n",
    "print(\"Are IDs normalized and continuous?\", unique_ids == list(range(len(unique_ids))))\n"
   ],
   "id": "f5407ae8371d8903",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 4: Re-ID Model with ResNet-50 backbone\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "\n",
    "class ReIDModel(nn.Module):\n",
    "    def __init__(self, embedding_dim=512, num_classes=751):\n",
    "        super(ReIDModel, self).__init__()\n",
    "\n",
    "        # Load ResNet-50\n",
    "        self.base = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "        self.base.fc = nn.Identity()  # remove the original FC\n",
    "\n",
    "        # Embedding layer (2048 -> 512)\n",
    "        self.embedding = nn.Linear(2048, embedding_dim)\n",
    "\n",
    "        # Optional classifier layer (for ID classification)\n",
    "        self.classifier = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.base(x)                  # shape: (batch_size, 2048)\n",
    "        embeddings = self.embedding(features)   # shape: (batch_size, 512)\n",
    "        logits = self.classifier(embeddings)    # for cross-entropy loss\n",
    "        return embeddings, logits\n"
   ],
   "id": "9d37d3b0e7484ff2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 5: Check model output\n",
    "model = ReIDModel(embedding_dim=512, num_classes=751)\n",
    "model.eval()\n",
    "\n",
    "sample_imgs, sample_labels, _ = next(iter(train_loader))\n",
    "with torch.no_grad():\n",
    "    embs, logits = model(sample_imgs)\n",
    "\n",
    "print(\"Input image shape:\", sample_imgs.shape)\n",
    "print(\"Embedding shape:\", embs.shape)\n",
    "print(\"Logits shape (for classification):\", logits.shape)\n"
   ],
   "id": "34c6dae8ed4ec94d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 6: Losses, optimizer, scheduler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Loss functions\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "triplet_loss_fn = nn.TripletMarginLoss(margin=0.3)\n",
    "\n",
    "# Optimizer (Adam works well)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "\n",
    "# Scheduler (optional)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.1)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ],
   "id": "52b0f99f11895063",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 7 (updated): Training loop with average loss metrics\n",
    "import time\n",
    "\n",
    "EPOCHS = 10\n",
    "use_triplet = True  # Toggle this as needed\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_ce = 0.0\n",
    "    total_tri = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for images, labels, _ in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = images.size(0)\n",
    "\n",
    "        # Forward pass\n",
    "        embeddings, logits = model(images)\n",
    "\n",
    "        # CrossEntropy loss\n",
    "        ce_loss = cross_entropy_loss(logits, labels)\n",
    "\n",
    "        # Triplet loss\n",
    "        if use_triplet:\n",
    "            label_to_indices = {}\n",
    "            for idx, label in enumerate(labels):\n",
    "                label = label.item()\n",
    "                label_to_indices.setdefault(label, []).append(idx)\n",
    "\n",
    "            triplets = []\n",
    "            for anchor_label, anchor_indices in label_to_indices.items():\n",
    "                if len(anchor_indices) < 2:\n",
    "                    continue\n",
    "                a, p = anchor_indices[0], anchor_indices[1]\n",
    "                for n_idx in range(len(labels)):\n",
    "                    if labels[n_idx].item() != anchor_label:\n",
    "                        triplets.append((a, p, n_idx))\n",
    "\n",
    "            if triplets:\n",
    "                a, p, n = zip(*triplets)\n",
    "                anchor = embeddings[list(a)]\n",
    "                positive = embeddings[list(p)]\n",
    "                negative = embeddings[list(n)]\n",
    "                tri_loss = triplet_loss_fn(anchor, positive, negative)\n",
    "            else:\n",
    "                tri_loss = torch.tensor(0.0).to(device)\n",
    "        else:\n",
    "            tri_loss = torch.tensor(0.0).to(device)\n",
    "\n",
    "        # Combine losses\n",
    "        loss = ce_loss + tri_loss\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate scaled losses\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_ce += ce_loss.item() * batch_size\n",
    "        total_tri += tri_loss.item() * batch_size\n",
    "        total_samples += batch_size\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}/{EPOCHS}] \"\n",
    "          f\"Avg Loss: {total_loss / total_samples:.4f} | \"\n",
    "          f\"Avg CE: {total_ce / total_samples:.4f} | \"\n",
    "          f\"Avg Triplet: {total_tri / total_samples:.4f} | \"\n",
    "          f\"Time: {time.time() - start:.1f}s\")\n"
   ],
   "id": "d703ddf2b60932e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 8–10: Extract features, match, visualize top-5 results\n",
    "from torchvision.utils import make_grid\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Re-load datasets (query + gallery)\n",
    "query_dir = os.path.join(DATA_DIR, \"query\")\n",
    "gallery_dir = os.path.join(DATA_DIR, \"bounding_box_test\")\n",
    "\n",
    "query_dataset = Market1501Dataset(query_dir, transform=transform)\n",
    "gallery_dataset = Market1501Dataset(gallery_dir, transform=transform)\n",
    "\n",
    "query_loader = DataLoader(query_dataset, batch_size=1, shuffle=True)\n",
    "gallery_loader = DataLoader(gallery_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Put model in eval mode\n",
    "model.eval()\n",
    "\n",
    "# Select a single query image\n",
    "query_img, query_label, _, = next(iter(query_loader))\n",
    "query_img = query_img.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    query_emb, _ = model(query_img)\n",
    "\n",
    "# Extract gallery embeddings\n",
    "gallery_embs = []\n",
    "gallery_paths = []\n",
    "\n",
    "for imgs, _, _ in gallery_loader:\n",
    "    imgs = imgs.to(device)\n",
    "    with torch.no_grad():\n",
    "        emb, _ = model(imgs)\n",
    "        gallery_embs.append(emb)\n",
    "gallery_embs = torch.cat(gallery_embs)\n",
    "gallery_embs = F.normalize(gallery_embs, p=2, dim=1)  # normalize for cosine similarity\n",
    "query_emb = F.normalize(query_emb, p=2, dim=1)\n",
    "\n",
    "# Compute cosine similarity\n",
    "distances = torch.mm(query_emb, gallery_embs.T).squeeze(0)  # shape: (num_gallery,)\n",
    "top5_indices = torch.topk(distances, k=5).indices.cpu().numpy()\n",
    "\n",
    "# Show top-5 matches\n",
    "plt.figure(figsize=(15, 3))\n",
    "plt.subplot(1, 6, 1)\n",
    "plt.imshow(query_img[0].permute(1, 2, 0).cpu().numpy() * 0.229 + 0.485)\n",
    "plt.title(\"Query\")\n",
    "plt.axis('off')\n",
    "\n",
    "for i, idx in enumerate(top5_indices):\n",
    "    path = gallery_dataset.img_paths[idx]\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    plt.subplot(1, 6, i+2)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Top {i+1}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "db1a200d53c4b13c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Match multiple query images (Top-5 each)\n",
    "num_queries_to_show = 5  # You can change this to 10, 20, etc.\n",
    "\n",
    "model.eval()\n",
    "shown = 0\n",
    "query_loader_vis = DataLoader(query_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "for query_img, query_label, query_cam in query_loader_vis:\n",
    "    if shown >= num_queries_to_show:\n",
    "        break\n",
    "\n",
    "    query_img = query_img.to(device)\n",
    "    query_label = query_label.item()\n",
    "    query_cam = query_cam.item()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        query_emb, _ = model(query_img)\n",
    "        query_emb = F.normalize(query_emb, p=2, dim=1)\n",
    "\n",
    "    # Compute gallery embeddings (cached would be better, but doing inline for now)\n",
    "    gallery_embs = []\n",
    "    gallery_labels = []\n",
    "    gallery_cams = []\n",
    "    gallery_paths = []\n",
    "\n",
    "    for imgs, labels, cams in gallery_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        with torch.no_grad():\n",
    "            emb, _ = model(imgs)\n",
    "            emb = F.normalize(emb, p=2, dim=1)\n",
    "        gallery_embs.append(emb)\n",
    "        gallery_labels.extend(labels.cpu().numpy())\n",
    "        gallery_cams.extend(cams.cpu().numpy())\n",
    "    gallery_embs = torch.cat(gallery_embs)\n",
    "    gallery_paths = gallery_dataset.img_paths\n",
    "\n",
    "    # Filter out same-camera same-person matches (optional)\n",
    "    filtered_indices = [\n",
    "        i for i, (pid, cam) in enumerate(zip(gallery_labels, gallery_cams))\n",
    "        if pid != query_label or cam != query_cam\n",
    "    ]\n",
    "\n",
    "    filtered_gallery = gallery_embs[filtered_indices]\n",
    "    filtered_paths = [gallery_paths[i] for i in filtered_indices]\n",
    "    filtered_pids = [gallery_labels[i] for i in filtered_indices]\n",
    "\n",
    "    # Compute similarity\n",
    "    distances = torch.mm(query_emb, filtered_gallery.T).squeeze(0)\n",
    "    top5_indices = torch.topk(distances, k=5).indices.cpu().numpy()\n",
    "    top5_paths = [filtered_paths[i] for i in top5_indices]\n",
    "    top5_labels = [filtered_pids[i] for i in top5_indices]\n",
    "\n",
    "    # Visualization\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    plt.subplot(1, 6, 1)\n",
    "    img = query_img[0].permute(1, 2, 0).cpu().numpy()\n",
    "    img = img * 0.229 + 0.485  # unnormalize\n",
    "    img = np.clip(img, 0, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Query\\nID {query_label}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    for i, (path, label) in enumerate(zip(top5_paths, top5_labels)):\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        plt.subplot(1, 6, i+2)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Top {i+1}\\nID {label}\")\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    shown += 1\n"
   ],
   "id": "dcb1b3c386b5fe11",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#  Load Underground ReID validation set with .jpeg support and custom ID parsing\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Custom dataset class for underground_reid\n",
    "class UndergroundReIDDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_paths = sorted(\n",
    "            glob(os.path.join(img_dir, \"*.jpg\")) +\n",
    "            glob(os.path.join(img_dir, \"*.jpeg\"))\n",
    "        )\n",
    "        self.transform = transform\n",
    "        self.raw_labels = []\n",
    "\n",
    "        for path in self.img_paths:\n",
    "            filename = os.path.basename(path)\n",
    "            try:\n",
    "                person_id = int(filename.split('_')[0])\n",
    "            except:\n",
    "                raise ValueError(f\"Cannot extract person ID from filename: {filename}\")\n",
    "            cam_id = -1  # Placeholder since underground_reid doesn't provide camera info\n",
    "            self.raw_labels.append((path, person_id, cam_id))\n",
    "\n",
    "        # Normalize IDs\n",
    "        self.unique_ids = sorted(set([pid for _, pid, _ in self.raw_labels]))\n",
    "        self.id2label = {pid: idx for idx, pid in enumerate(self.unique_ids)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, pid, cam = self.raw_labels[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        label = self.id2label[pid]\n",
    "        return img, label, cam\n",
    "\n",
    "# 🔗 Define folder paths\n",
    "underground_query_dir = os.path.join(\"data\", \"underground_reid\", \"probe\")\n",
    "underground_gallery_dir = os.path.join(\"data\", \"underground_reid\", \"gallery\")\n",
    "\n",
    "# 📦 Load datasets and create loaders\n",
    "underground_query_dataset = UndergroundReIDDataset(underground_query_dir, transform=transform)\n",
    "underground_gallery_dataset = UndergroundReIDDataset(underground_gallery_dir, transform=transform)\n",
    "\n",
    "underground_query_loader = DataLoader(underground_query_dataset, batch_size=1, shuffle=False)\n",
    "underground_gallery_loader = DataLoader(underground_gallery_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 🟢 Switch into evaluation mode\n",
    "query_loader = underground_query_loader\n",
    "gallery_loader = underground_gallery_loader\n",
    "\n",
    "#  Check loaded counts\n",
    "print(f\" Underground ReID loaded: {len(underground_query_dataset)} queries, {len(underground_gallery_dataset)} gallery images.\")\n"
   ],
   "id": "667112f5f430638",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "# Helper to unnormalize images for display\n",
    "def unnormalize(img_tensor):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    return img_tensor * std + mean\n",
    "\n",
    "# Pick a query image\n",
    "query_img, query_label, _ = next(iter(underground_query_loader))\n",
    "query_img = query_img.to(device)\n",
    "query_label = query_label.item()\n",
    "\n",
    "# Extract query embedding\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    query_emb, _ = model(query_img)\n",
    "    query_emb = F.normalize(query_emb, p=2, dim=1)\n",
    "\n",
    "# Extract all gallery embeddings\n",
    "gallery_embs = []\n",
    "gallery_imgs = []\n",
    "gallery_labels = []\n",
    "\n",
    "for imgs, labels, _ in underground_gallery_loader:\n",
    "    imgs = imgs.to(device)\n",
    "    with torch.no_grad():\n",
    "        emb, _ = model(imgs)\n",
    "        emb = F.normalize(emb, p=2, dim=1)\n",
    "    gallery_embs.append(emb)\n",
    "    gallery_imgs.append(imgs.cpu())\n",
    "    gallery_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "gallery_embs = torch.cat(gallery_embs)\n",
    "gallery_imgs = torch.cat(gallery_imgs)\n",
    "\n",
    "# Compute similarity\n",
    "similarities = torch.mm(query_emb, gallery_embs.T).squeeze(0)\n",
    "top5_indices = torch.topk(similarities, k=5).indices.cpu().numpy()\n",
    "\n",
    "# Display results\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Query\n",
    "plt.subplot(1, 6, 1)\n",
    "query_disp = unnormalize(query_img.squeeze(0).cpu())\n",
    "plt.imshow(query_disp.permute(1, 2, 0).numpy())\n",
    "plt.title(\"Query\")\n",
    "plt.axis('off')\n",
    "\n",
    "# Top 5 gallery\n",
    "for i, idx in enumerate(top5_indices):\n",
    "    plt.subplot(1, 6, i + 2)\n",
    "    gal_disp = unnormalize(gallery_imgs[idx])\n",
    "    plt.imshow(gal_disp.permute(1, 2, 0).numpy())\n",
    "    plt.title(f\"Top {i+1}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "855c2da19e495d63",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
